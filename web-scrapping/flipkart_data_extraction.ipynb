{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01efe58f",
   "metadata": {},
   "source": [
    "## Web scraping tool using chroma driver , scraping flipkart website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e0928",
   "metadata": {},
   "source": [
    "**Import Key Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813dd80d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27733d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from selenium import webdriver                                      # Purpose: Loads the browser driver (e.g., Chrome, Firefox) to automate web browsing tasks\n",
    "from selenium.webdriver.common.by import By                         # Purpose: Provides convenient ways to locate elements, such as By.ID, By.CLASS_NAME, By.XPATH, etc.\n",
    "from selenium.webdriver.support.ui import WebDriverWait             # Purpose: Allows you to wait for a certain condition (like element visibility) before proceeding. Used to deal with dynamic content loading.\n",
    "from selenium.webdriver.support import expected_conditions as EC    # Purpose: Used with WebDriverWait to specify what condition to wait for â€” like an element being present, clickable, etc.\n",
    "import time                                                         # Purpose: Provides manual delays using time.sleep() when needed.\n",
    "from bs4 import BeautifulSoup                                       # Purpose: Parses HTML content fetched from the website. Commonly used to extract structured data (like titles, prices, links).\n",
    "import lxml                                                         # Purpose: A high-performance XML/HTML parser used by BeautifulSoup for faster and more accurate parsing.\n",
    "import pandas as pd                                                 # Purpose: Provides powerful data manipulation capabilities. You can store scraped data in a DataFrame and export it to CSV, Excel, etc.\n",
    "import re                                                           # Purpose: Enables regular expression matching. Used to extract text or clean/validate data.\n",
    "from datetime import datetime                                       # Purpose: Helps in working with date and time, such as tagging scraped data with a timestamp.\n",
    "from selenium.webdriver.common.keys import Keys                     # Purpose: Allows you to simulate keyboard keys like ENTER, TAB, ESC, etc., during automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac50516",
   "metadata": {},
   "source": [
    "| Module                   | Purpose                           |\n",
    "| ------------------------ | --------------------------------- |\n",
    "| `selenium.webdriver`     | Automates browsers                |\n",
    "| `By`                     | Element location strategy         |\n",
    "| `WebDriverWait` + `EC`   | Dynamic waiting for page elements |\n",
    "| `time`                   | Static wait or delay              |\n",
    "| `BeautifulSoup` + `lxml` | Parse and navigate HTML           |\n",
    "| `pandas`                 | Data storage and export           |\n",
    "| `re`                     | Text extraction and cleaning      |\n",
    "| `datetime`               | Timestamps                        |\n",
    "| `Keys`                   | Simulating key presses            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478e793",
   "metadata": {},
   "source": [
    "### Step1: Get all product Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a7f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start Time: 15:55:25.624962 ---------------------------> \n",
      "Waiting for search input...\n",
      "Typing in search input...\n",
      "Submitting search form...\n",
      "Waiting for search results...\n",
      "Collecting pagination links...\n",
      "Pagination Links Count: 25\n",
      "All Pagination Links:  ['https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=1', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=4', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=5', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=6', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=7', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=8', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=9', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=10', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=11', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=12', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=13', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=14', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=15', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=16', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=17', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=18', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=19', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=20', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=21', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=22', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=23', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=24', 'https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=25']\n",
      "Collecting Product Detail Page Links\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=1 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=4 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=5 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=6 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=7 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=8 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=9 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=10 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=11 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=12 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=13 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=14 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=15 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=16 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=17 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=18 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=19 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=20 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=21 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=22 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=23 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=24 Done ------>\n",
      "https://www.flipkart.com/search?q=sports+shoes+for+women&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=25 Done ------>\n",
      "All Product Detail Page Links Captured:  1000\n",
      "Total Product Detail Page Links 1000\n",
      "Session End Time: 15:56:19.284516 ---------------------------> \n"
     ]
    }
   ],
   "source": [
    "# ğŸ§¾ Section 1: Setup & Inputs\n",
    "#Inputs to search\n",
    "search_box_text = 'sports shoes for women'                                                                                  # search_box_text: The keyword to search on Flipkart.   \n",
    "website_link = 'https://www.flipkart.com/'                                                                                  # website_link: The Flipkart homepage URL.\n",
    "\n",
    "#initiating the browser\n",
    "#session start time\n",
    "session_start_time = datetime.now().time()\n",
    "print(f\"Session Start Time: {session_start_time} ---------------------------> \")                                            # Stores the time the scraping session begins (used for logging).\n",
    "\n",
    "# ğŸš€ Section 2: Browser Initialization\n",
    "#starting the browser\n",
    "driver = webdriver.Chrome()                                                                                                 # Launches a new Chrome browser session and opens Flipkart's website.   \n",
    "driver.get(website_link)                                                                                                    # Maximizes the browser for better visibility and scraping.\n",
    "driver.maximize_window()\n",
    "\n",
    "# ğŸ” Section 3: Perform the Search\n",
    "print('Waiting for search input...')                                                                                                 \n",
    "search_input = WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[autocomplete=\"off\"]')))  # Waits up to 120 seconds for the search box to appear.\n",
    "        \n",
    "print('Typing in search input...') \n",
    "search_input.send_keys(search_box_text) \n",
    "        \n",
    "print('Submitting search form...') \n",
    "search_input.send_keys(Keys.RETURN)                                                                                         # Sends the search keyword and presses ENTER.\n",
    "        \n",
    "print('Waiting for search results...') \n",
    "WebDriverWait(driver, 120).until( EC.presence_of_element_located((By.CSS_SELECTOR, '[target=\"_blank\"]')) )                  # Waits for at least one search result to be present after submitting the query.\n",
    "\n",
    "print('Collecting pagination links...') \n",
    "\n",
    "# ğŸ“„ Section 4: Handle Pagination\n",
    "#we want first 25 pages [pagination link]  [1000 Products]\n",
    "#logic: Let's get the first page pagination link and append the number in the end for 25 pages and store in a list\n",
    "all_pagination_links =[]\n",
    "\n",
    "first_page = driver.find_elements(By.CSS_SELECTOR, 'nav a')[0]                                                              # Gets the pagination link of the first page of results.\n",
    "first_page_link = first_page.get_attribute('href')                                                                    \n",
    "all_pagination_links.append(first_page_link)                                                                                \n",
    "\n",
    "for i in range(2, 26):                                                                                                      # Constructs pagination links for pages 2 to 25 by editing the last digit of the first page URL.\n",
    "    new_pagination_link = first_page_link[: -1] + str(i)\n",
    "    all_pagination_links.append(new_pagination_link)\n",
    "\n",
    "print('Pagination Links Count:', len(all_pagination_links)) \n",
    "print(\"All Pagination Links: \", all_pagination_links)\n",
    "\n",
    "\n",
    "print(\"Collecting Product Detail Page Links\")\n",
    "all_product_links = []\n",
    "\n",
    "\n",
    "# ğŸ“¦ Section 5: Collect All Product Links\n",
    "for link in all_pagination_links:                                                                             \n",
    "    driver.get(link)\n",
    "    # Wait for the page to load by checking document.readyState                                                              # Visits each pagination page.\n",
    "    WebDriverWait(driver, 120).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "    #wait until elements located                                                                                             # Waits until the page is fully loaded.\n",
    "    WebDriverWait(driver, 120).until(EC.presence_of_element_located((By.CLASS_NAME, 'rPDeLR')))                              # Finds all product cards using class name rPDeLR (Flipkart's internal class for product tiles).\n",
    "                \n",
    "    all_products = driver.find_elements(By.CLASS_NAME, 'rPDeLR')\n",
    "    all_links = [element.get_attribute('href') for element in all_products]                                                  # Collects the href link from each product and adds it to the list.\n",
    "\n",
    "    print(f\"{link} Done ------>\")\n",
    "\n",
    "    all_product_links.extend(all_links)\n",
    "    \n",
    "print('All Product Detail Page Links Captured: ', len(all_product_links)) \n",
    "\n",
    "# ğŸ§¾ Section 6: Save the Product Links\n",
    "# Creating a DataFrame from the list\n",
    "df_product_links = pd.DataFrame(all_product_links, columns=['product_links'])                                                # Converts the product links into a Pandas DataFrame.\n",
    "#remove any duplicates\n",
    "df_product_links = df_product_links.drop_duplicates(subset=['product_links'])                                                # Removes duplicate URLs.\n",
    "\n",
    "print(\"Total Product Detail Page Links\", len(df_product_links))                                                              # Saves all links to a CSV file named flipkart_product_links.csv.\n",
    "df_product_links.to_csv('flipkart_product_links.csv', index = False)\n",
    "\n",
    "# âœ… Section 7: Cleanup\n",
    "driver.close()                                                                                                               # Closes the browser session.\n",
    "session_end_time = datetime.now().time()\n",
    "print(f\"Session End Time: {session_end_time} ---------------------------> \")                                                 # Logs the time when scraping ended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef928ca",
   "metadata": {},
   "source": [
    "| Step | Action                                        |\n",
    "| ---- | --------------------------------------------- |\n",
    "| 1    | Open Flipkart and search for a keyword        |\n",
    "| 2    | Wait for results and paginate across 25 pages |\n",
    "| 3    | Collect all product detail page URLs          |\n",
    "| 4    | Save the results in a CSV file                |\n",
    "| 5    | Log session start and end time                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a19ab",
   "metadata": {},
   "source": [
    "### Step2: Get Individual product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62d9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start Time: 15:57:55.357811 ---------------------------> \n",
      "Collecting Individual Product Detail Information\n",
      "URL 1 completed *******\n",
      "URL 2 completed *******\n",
      "URL 3 completed *******\n",
      "URL 4 completed *******\n",
      "URL 5 completed *******\n",
      "URL 6 completed *******\n",
      "URL 7 completed *******\n",
      "URL 8 completed *******\n",
      "URL 9 completed *******\n",
      "URL 10 completed *******\n",
      "Total product pages scrapped:  10\n",
      "Final Total Products:  9\n",
      "Total Unavailable Products :  0\n",
      "Total Duplicate Products:  1\n",
      "Session End Time: 15:58:13.192908 ---------------------------> \n"
     ]
    }
   ],
   "source": [
    "# ğŸ” Goal : Scrape detailed information (brand, title, price, discount, rating, reviews) for individual products from Flipkart using product URLs stored in a CSV file.\n",
    "\n",
    "# ğŸ“Œ 1. Setup & Inputs\n",
    "#session start time\n",
    "session_start_time = datetime.now().time()\n",
    "print(f\"Session Start Time: {session_start_time} ---------------------------> \")                                    # Captures the start time of the scraping session.\n",
    "\n",
    "\n",
    "#reading the csv file which contain all product links\n",
    "df_product_links = pd.read_csv(\"flipkart_product_links.csv\")\n",
    "\n",
    "# Remove the below line to scrap all the products. For demonstration purpose we are scraping only 10 products       # Loads product links from the CSV and selects only the top 10 (for demo/testing).\n",
    "df_product_links = df_product_links.head(10)\n",
    "\n",
    "all_product_links = df_product_links['product_links'].tolist()                                                      # Converts the DataFrame column to a Python list.\n",
    "print(\"Collecting Individual Product Detail Information\")\n",
    "\n",
    "#ğŸš€ 2. Start WebDriver\n",
    "#starting the browser\n",
    "driver = webdriver.Chrome()                                                                                         # Starts a new instance of the Chrome browser\n",
    "\n",
    "# ğŸ”„ 3. Scraping Loop Initialization                                                                                # Prepares variables to store results, failures, and counters.\n",
    "complete_product_details = []\n",
    "unavailable_products = []\n",
    "successful_parsed_urls_count = 0\n",
    "complete_failed_urls_count = 0\n",
    "\n",
    "# ğŸ§ª 4. Main Loop: Scrape Each Product Page\n",
    "for product_page_link in all_product_links:                                                                         # Opens each product link and waits for the page to fully load.                                                    \n",
    "    try: \n",
    "        driver.get(product_page_link)\n",
    "    \n",
    "        # Wait for the page to load by checking document.readyState\n",
    "        WebDriverWait(driver, 120).until(lambda d: d.execute_script('return document.readyState') == 'complete')\n",
    "    \n",
    "        WebDriverWait(driver, 120).until( EC.presence_of_element_located((By.CSS_SELECTOR, '[target=\"_blank\"]')))\n",
    "\n",
    "# âŒ 5. Check for \"Unavailable\" Products       \n",
    "        #checking if product is available or not                                                                    # Looks for a â€œSold Outâ€ or â€œCurrently Unavailableâ€ message. If found, logs and skips.                       \n",
    "        try:\n",
    "            product_status =  driver.find_element(By.CLASS_NAME, 'Z8JjpR').text\n",
    "            if product_status == 'Currently Unavailable' or product_status == 'Sold Out':\n",
    "                unavailable_products.append(product_page_link)\n",
    "                successful_parsed_urls_count += 1\n",
    "                print(f\"URL {successful_parsed_urls_count} completed --->\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ğŸ·ï¸ 6. Extract Key Fields    \n",
    "        #brand\n",
    "        brand =  driver.find_element(By.CLASS_NAME, 'mEh187').text\n",
    "    \n",
    "        #title   ::  Title (with color/extra info removed)   \n",
    "        title = driver.find_element(By.CLASS_NAME, 'VU-ZEz').text\n",
    "        title = re.sub(r'\\s*\\([^)]*\\)', '', title)  #removing data withing parenthesis (color information)\n",
    "    \n",
    "        #price      \n",
    "        price = driver.find_element(By.CLASS_NAME, 'Nx9bqj').text\n",
    "        price = re.findall(r'\\d+', price)\n",
    "        price = ''.join(price)\n",
    "    \n",
    "        # Discount  \n",
    "        try:\n",
    "            discount = driver.find_element(By.CLASS_NAME, 'UkUFwK').text\n",
    "            discount = re.findall(r'\\d+', discount)\n",
    "            discount = ''.join(discount)\n",
    "            discount = int(discount) / 100\n",
    "        except:\n",
    "            discount = ''\n",
    "    \n",
    "        #for a new product, there will be no avg_rating and total_ratings                                       # Rating & Total Ratings\n",
    "        try:\n",
    "            product_review_status = driver.find_element(By.CLASS_NAME, 'E3XX7J').text\n",
    "            if product_review_status == 'Be the first to Review this product':\n",
    "                avg_rating = ''\n",
    "                total_ratings = ''\n",
    "        except:\n",
    "            avg_rating = driver.find_element(By.CLASS_NAME, 'XQDdHH').text\n",
    "            total_ratings = driver.find_element(By.CLASS_NAME, 'Wphh3N').text.split(' ')[0]\n",
    "            #remove the special character\n",
    "            if ',' in total_ratings:\n",
    "                total_ratings = int(total_ratings.replace(',', ''))\n",
    "            else:\n",
    "                total_ratings = int(total_ratings)\n",
    "    \n",
    "        successful_parsed_urls_count += 1\n",
    "        print(f\"URL {successful_parsed_urls_count} completed *******\")\n",
    "        complete_product_details.append([product_page_link, title, brand, price, discount, avg_rating, total_ratings])    # âœ… 7. Store Results\n",
    "    except Exception as e:                                                                                                # â— 8. Handle Exceptions\n",
    "        print(f\"Failed to establish a connection for URL {product_page_link}:  {e}\")\n",
    "        unavailable_products.append(product_page_link)\n",
    "        complete_failed_urls_count += 1\n",
    "        print(f\"Failed URL Count {complete_failed_urls_count}\")\n",
    "\n",
    "# ğŸ“Š 9. Create DataFrames\n",
    "#create pandas dataframe \n",
    "df = pd.DataFrame(complete_product_details, columns = ['product_link', 'title', 'brand', 'price', 'discount', 'avg_rating', 'total_ratings'])\n",
    "#duplicates processing\n",
    "df_duplicate_products = df[df.duplicated(subset=['brand', 'price', 'discount', 'avg_rating', 'total_ratings'])]\n",
    "df = df.drop_duplicates(subset=['brand', 'price', 'discount', 'avg_rating', 'total_ratings'])\n",
    "#unavailable products\n",
    "df_unavailable_products = pd.DataFrame(unavailable_products, columns=['link'])\n",
    "\n",
    "\n",
    "#prining the stats\n",
    "print(\"Total product pages scrapped: \", len(all_product_links))\n",
    "print(\"Final Total Products: \", len(df))\n",
    "print(\"Total Unavailable Products : \", len(df_unavailable_products))\n",
    "print(\"Total Duplicate Products: \", len(df_duplicate_products))\n",
    "\n",
    "# ğŸ’¾ 10. Save to CSV\n",
    "#saving all the files\n",
    "df.to_csv('flipkart_product_data.csv', index = False)\n",
    "df_unavailable_products.to_csv('unavailable_products.csv', index = False)\n",
    "df_duplicate_products.to_csv('duplicate_products.csv', index = False)\n",
    "\n",
    "# ğŸ›‘ 11. Close Driver & Print Stats\n",
    "driver.close()\n",
    "session_end_time = datetime.now().time()\n",
    "print(f\"Session End Time: {session_end_time} ---------------------------> \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45947bf5",
   "metadata": {},
   "source": [
    "| Task       | Description                                 |\n",
    "| ---------- | ------------------------------------------- |\n",
    "| ğŸ Start   | Load links & start session                  |\n",
    "| ğŸ”„ Loop    | Visit each product page                     |\n",
    "| ğŸ“Œ Extract | Brand, Title, Price, Discount, Ratings      |\n",
    "| âŒ Handle  | Unavailable or Failed Pages                |\n",
    "| ğŸ“¦ Store   | Product data in DataFrame                   |\n",
    "| ğŸ’¾ Save    | Output to `flipkart_product_data.csv`, etc. |\n",
    "| âœ… Clean   | Remove duplicates                          |\n",
    "| ğŸ›‘ Finish  | End session & close browser                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c200d9c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
